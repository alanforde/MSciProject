{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## User data preparation for clustering\n",
    "orders = pd.read_csv(\"Instacart_data/orders.csv\")\n",
    "countById = orders.groupby(['user_id']).count()\n",
    "meanById = orders.groupby(['user_id']).mean()\n",
    "userCountDF = countById[['order_id']].copy().rename(index=str, columns={\"order_id\": \"order_count\"})\n",
    "userMeanDF = meanById[['order_dow', 'order_hour_of_day', 'days_since_prior_order']].copy().rename(index=str, columns={'order_dow' : 'avg_order_day', 'order_hour_of_day': 'avg_order_hour', 'days_since_prior_order' : 'avg_days_between_orders'}).round(0)\n",
    "userDF = userCountDF.join(userMeanDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## KMeans Clustering\n",
    "kmeans = KMeans(n_clusters=5, random_state=0)\n",
    "kmeans.fit(userDF)\n",
    "labels = kmeans.labels_\n",
    "userDF = userDF.join(pd.DataFrame(labels, index=userDF.index, columns=['cluster']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create list of products, each product represented as a list of words (create list of lists)\n",
    "products = pd.read_csv(\"Instacart_data/products.csv\")\n",
    "productNames = products[['product_name']].values\n",
    "splitProducts = []\n",
    "for product in productNames:\n",
    "    productName = product[0].lower()\n",
    "    productList = productName.split()\n",
    "    splitProducts += [productList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products_aisle_dept = products[['product_id','aisle_id', 'department_id']]\n",
    "products_aisle_dept.set_index('product_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get embeddings for each product by averaging embeddings of component words, store in dataframe\n",
    "def dict_retriever(product):\n",
    "    if product.lower() in prodVectorDict:\n",
    "        return prodVectorDict[product.lower()]\n",
    "    else:\n",
    "        return np.ones(300)\n",
    "\n",
    "prodVectorDict = {}\n",
    "for product in splitProducts:\n",
    "    if product[0] in model.vocab:\n",
    "        combinedVector = model[product[0]]\n",
    "    else:\n",
    "        combinedVector = combinedVector + np.ones(300)\n",
    "    for word in product[1:]:\n",
    "        if word in model.vocab:\n",
    "            combinedVector = combinedVector + model[word]\n",
    "        else:\n",
    "            combinedVector = combinedVector + np.ones(300)\n",
    "    averageVector = combinedVector/len(product)\n",
    "    productString = ' '.join(product)\n",
    "    prodVectorDict[productString] = averageVector\n",
    "prodVectorDF = pd.DataFrame(products[['product_name', 'product_id']])\n",
    "prodVectorDF['product_embedding'] = prodVectorDF['product_name'].apply(lambda x: dict_retriever(x))\n",
    "prodVectorDF = prodVectorDF.set_index('product_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prodVectorDF = prodVectorDF.join(products_aisle_dept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code to generate subcart embeddings for all our known subcarts\n",
    "\n",
    "\n",
    "## Function taken and slightly adapted from https://gist.github.com/jlln/338b4b0b55bd6984f883.\n",
    "def splitDataFrameList(df,target_column,separator):\n",
    "    ''' df = dataframe to split,\n",
    "    target_column = the column containing the values to split\n",
    "    separator = the symbol used to perform the split\n",
    "    returns: a dataframe with each entry for the target column separated, with each element moved into a new row. \n",
    "    The values in the other columns are duplicated across the newly divided rows.\n",
    "    '''\n",
    "    def splitListToRows(row,row_accumulator,target_column,separator):\n",
    "        split_row = row[target_column]\n",
    "        for s in split_row:\n",
    "            new_row = row.to_dict()\n",
    "            new_row[target_column] = s\n",
    "            row_accumulator.append(new_row)\n",
    "    new_rows = []\n",
    "    df.apply(splitListToRows,axis=1,args = (new_rows,target_column,separator))\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    return new_df\n",
    "\n",
    "orderDetailsDF = pd.read_csv(\"Instacart_data/order_products__prior.csv\")\n",
    "suborderDetailsDF = pd.DataFrame(orderDetailsDF.groupby('order_id')['product_id'].apply(list))\n",
    "suborderDetailsDF['product_id'] = suborderDetailsDF['product_id'].map(lambda x: x[:len(x)-1])\n",
    "suborderDetailsDF = suborderDetailsDF.reset_index()\n",
    "suborderDetailsDF = splitDataFrameList(suborderDetailsDF,'product_id', ',')\n",
    "suborderDetailsDF = suborderDetailsDF.join(prodVectorDF, on=\"product_id\")\n",
    "subcartEmbeddingDF = pd.DataFrame(suborderDetailsDF.groupby(['order_id'])['product_embedding'].apply(np.mean))\n",
    "subcartEmbeddingDF = subcartEmbeddingDF.rename(columns={\"product_embedding\": \"subcart_embedding\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read and format positive training examples\n",
    "orders = pd.read_csv(\"Instacart_data/orders.csv\")\n",
    "countById = orders.groupby(['user_id']).count()\n",
    "frequentBuyers = countById[countById['order_id'] >= 10]   ## Only users with more than 10 purchases considered\n",
    "sampleOrders = pd.read_csv(\"Instacart_data/order_products__prior.csv\")\n",
    "userSample50 = frequentBuyers.sample(n=30000)             ## Sample users to reduce size of dataset\n",
    "userSample = orders[orders.user_id.isin(userSample50.index.values)]\n",
    "orderDF = pd.DataFrame(sampleOrders.groupby('order_id')['product_id'].apply(list))\n",
    "orderDF = orderDF.rename(index=str, columns={\"product_id\": \"order\"})\n",
    "orderDF.index = orderDF.index.map(int)\n",
    "usersDF = pd.read_csv(\"Instacart_data/orders.csv\")\n",
    "usersDF = usersDF[['order_id', 'user_id']].set_index('order_id')\n",
    "usersDF.index = usersDF.index.map(int)\n",
    "userOrderDF = orderDF.join(usersDF, how=\"inner\")\n",
    "userOrderDF['keeporder'] = userOrderDF['order'].map(lambda x: len(x))\n",
    "userOrderDF = userOrderDF[userOrderDF['keeporder'] > 10]\n",
    "userOrderDF = userOrderDF.drop(['keeporder'], axis=1)\n",
    "userOrderDF['suborder'] = userOrderDF['order'].map(lambda x: x[:len(x)-1])\n",
    "userOrderDF['next_item'] = userOrderDF['order'].map(lambda x: x[len(x)-1])\n",
    "userOrderDF['label'] = [True] * len(userOrderDF)\n",
    "userOrderDF.reset_index(inplace=True)\n",
    "userOrderDF.rename(index=str, columns={\"index\": \"order_id\"},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## These 100 true samples are used later for pytrec, removing here was the easiest way\n",
    "true_subset = userOrderDF.sample(n=100, random_state=42)\n",
    "userOrderDF.drop(true_subset.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Negative sampling by randomly assigning false next_items at a ratio of 4:1 with true samples\n",
    "products = pd.read_csv(\"Instacart_data/products.csv\")\n",
    "productlist = products['product_id'].tolist()\n",
    "userOrderDF = userOrderDF.reset_index()\n",
    "falseExamples = pd.DataFrame()\n",
    "falseExamples = falseExamples.append([userOrderDF]*4)\n",
    "falseExamples = falseExamples.rename(index=str, columns={\"next_item\": \"correct_item\"})\n",
    "falseExamples['next_item'] = falseExamples['correct_item'].map(lambda x: np.random.choice(productlist, 1) )\n",
    "falseExamples['label'] = [False] * len(falseExamples)\n",
    "falseExamples.drop(['correct_item'], axis=1, inplace=True)\n",
    "falseExamples['next_item'] = falseExamples['next_item'].map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Do before pickling\n",
    "falseExamples.reset_index(inplace=True)\n",
    "falseExamples.drop(['index','level_0'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "falseDict = falseExamples.to_dict()\n",
    "with open('false.pickle', 'wb') as handle:\n",
    "    pickle.dump(falseDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('false.pickle', 'rb') as handle:\n",
    "    falseDict = pickle.load(handle)\n",
    "    \n",
    "falseExamples = pd.DataFrame.from_dict(falseDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## RERUN EARLY CELLS AND RESTART THIS, SHOULD BE GOOD FOR LATER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create dataset by joining positive and negative samples\n",
    "datasetDF = pd.concat([falseExamples,userOrderDF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasetDF.drop(['index', 'level_0'], axis=1, inplace=True)\n",
    "true_subset.drop(['index', 'level_0'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Append features relating to user\n",
    "datasetDF['user_id'] = pd.to_numeric(datasetDF['user_id'])\n",
    "userDF.index = pd.to_numeric(userDF.index)\n",
    "userFeatures = datasetDF.join(userDF, on=\"user_id\", how=\"inner\")\n",
    "userFeatures.rename(index=str, columns={\"cluster\": \"user_cluster\"},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Append features relating to next product\n",
    "\n",
    "## Just to create column names for embedding values\n",
    "column_headers= []\n",
    "for i in range(1,301):\n",
    "    header = \"prod_embed_\" + str(i)\n",
    "    column_headers += [header]\n",
    "    \n",
    "    \n",
    "prodVectorDF = prodVectorDF.drop(['product_name'], axis=1)\n",
    "prodVectorDF[column_headers] = pd.DataFrame(prodVectorDF.product_embedding.values.tolist(), index= prodVectorDF.index)\n",
    "prodVectorDF = prodVectorDF.drop(['product_embedding'], axis=1)\n",
    "userProdFeatures = userFeatures.join(prodVectorDF, on=\"next_item\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Append features relating to current cart (subcart)\n",
    "\n",
    "cart_column_headers= []\n",
    "for i in range(1,301):\n",
    "    header = \"subcart_embed_\" + str(i)\n",
    "    cart_column_headers += [header]\n",
    "    \n",
    "userProdFeatures['order_id'] = pd.to_numeric(userProdFeatures['order_id'])\n",
    "subcartEmbeddingDF[cart_column_headers] = pd.DataFrame(subcartEmbeddingDF.subcart_embedding.values.tolist(), index= subcartEmbeddingDF.index)\n",
    "subcartEmbeddingDF = subcartEmbeddingDF.drop(['subcart_embedding'], axis=1)\n",
    "subEmbSamp = subcartEmbeddingDF.sample(320000, random_state=42)   ## Random sampling just to reduce size of dataset\n",
    "finalDataset = userProdFeatures.join(subEmbSamp, on=\"order_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Tidy up final dataset and extract labels\n",
    "#finalDataset['label'] = finalDataset['label'].map(lambda x: 1 if x else 0)\n",
    "labels = finalDataset['label']\n",
    "finalDataset.drop(['next_item', 'order', 'suborder', 'order_id', 'user_id','label'], axis=1, inplace=True)\n",
    "finalDataset = finalDataset.reset_index()\n",
    "finalDataset.drop(['index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## split generated data into train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(finalDataset, labels, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "## Fit MLP neural network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nn = MLPClassifier(max_iter=30).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "## Fit Logistic Regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "## Fit LinearSVM classifier\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.SGDClassifier(loss=\"hinge\").fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Generate dataset for pytrec evaluation, its cumbersome doing this twice but ensures\n",
    "## test samples are totally separate than training samples\n",
    "false_subset = pd.DataFrame()\n",
    "false_subset = false_subset.append([true_subset]*100)\n",
    "false_subset['next_item'] = false_subset['next_item'].map(lambda x: np.random.choice(productlist, 1))\n",
    "false_subset['label'] = [False] * len(false_subset)\n",
    "false_subset['next_item'] = false_subset['next_item'].map(lambda x: x[0])\n",
    "pytrecDataset = pd.concat([false_subset,true_subset])\n",
    "pytrecDataset.reset_index(inplace=True)\n",
    "pytrecDataset.drop(['index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pytrec_eval/logreg_run.pickle', 'rb') as handle:\n",
    "    run = pickle.load(handle, encoding='latin1')\n",
    "with open('pytrec_eval/logreg_qrel.pickle', 'rb') as handle:\n",
    "    qrel = pickle.load(handle, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderids = np.unique(pytrecDataset.order_id.values)\n",
    "qrel = {}\n",
    "run = {}\n",
    "for i in range(0, len(orderids)):\n",
    "    thisTest = pytrecDataset.loc[pytrecDataset[\"order_id\"] == orderids[i]]\n",
    "    thisTest['user_id'] = pd.to_numeric(thisTest['user_id'])\n",
    "    userDF.index = pd.to_numeric(userDF.index)\n",
    "    withUserFeatures = thisTest.join(userDF, on=\"user_id\", how=\"inner\")\n",
    "    withProdFeatures = withUserFeatures.join(prodVectorDF, on=\"next_item\", how=\"inner\")\n",
    "    thisFinal = withProdFeatures.join(subcartEmbeddingDF, on=\"order_id\", how=\"inner\")\n",
    "    thisFinal['label'] = thisFinal['label'].map(lambda x: 1 if x else 0)\n",
    "    thisFinalLabels = thisFinal['label']\n",
    "    thisFinal = thisFinal.drop(['next_item', 'order', 'suborder', 'order_id', 'user_id','label'], axis=1)\n",
    "    ##preds = clf.decision_function(thisFinal)\n",
    "    preds = logreg.predict_proba(thisFinal)\n",
    "    #preds = nn.predict_proba(thisFinal)\n",
    "    confidence_dict = {}\n",
    "    truth_dict = {}\n",
    "    for x in range(0,len(preds)):\n",
    "        confidence_dict[str(x)] = abs(preds[x][1])\n",
    "        truth_dict[str(x)] = int(thisFinalLabels.values[x])\n",
    "    qrel[str(i)] = truth_dict\n",
    "    run[str(i)] = confidence_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================Results=============================\n",
      "Average NDCG: 0.4958237129703898\n",
      "Average MAP: 0.3705469495256167\n",
      "==============================================================\n"
     ]
    }
   ],
   "source": [
    "import pytrec_eval\n",
    "import json\n",
    "\n",
    "def Average(lst): \n",
    "    return sum(lst) / len(lst)\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrel,{'map', 'ndcg'})\n",
    "results = evaluator.evaluate(run)\n",
    "NDCGlist = []\n",
    "MAPlist = []\n",
    "for key in results:\n",
    "    result = results[key]\n",
    "    NDCGlist += [result['ndcg']]\n",
    "    MAPlist += [result['map']]\n",
    "print (\"==========================Results=============================\")\n",
    "print (\"Average NDCG: \" + str(Average(NDCGlist)))\n",
    "print (\"MAP: \" + str(Average(MAPlist)))\n",
    "print (\"==============================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
